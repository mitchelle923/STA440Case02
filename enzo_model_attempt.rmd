---
title: "Attempting a Model"
author: "Enzo Moraes Mescall"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(tidyverse)
library(invgamma)
library(fitdistrplus)
```

## Reading data

```{r data}
data_23 = read.csv("Cleaned Data/modified_2023.csv") %>%
  mutate(Apparatus = toupper(Apparatus))
data_21 = read.csv("Cleaned Data/data_2017_2021.csv") 
```

```{r}
# Plotting distributions of difficulty scores add more ticks on the x axis
data_23 %>%
  ggplot(aes(x = D_Score, fill = Apparatus)) +
  geom_histogram() +
  labs(title = "Distribution of Difficulty Scores",
       x = "Difficulty Score",
       y = "Count") +
  facet_wrap(~Apparatus)
```


```{r}
# Plotting distributions of execution scores
data_23 %>%
  ggplot(aes(x = E_Score, fill = Apparatus)) +
  geom_histogram() +
  labs(title = "Distribution of Execution Scores",
       x = "Execution Score",
       y = "Count") +
  facet_wrap(~Apparatus)
```

Its clear that the execution scores have a left tail to them while the execution scores are more unimodal. Although it doesn't seem like the difficulty scores are exactly normal, they are more normal than the execution scores. 

```{r}
calculate_p_value <- function(estimate, se, null_value = 0) {
  z <- (estimate - null_value) / se
  p_value <- 2 * pnorm(-abs(z))
  return(p_value)
}
```

```{r}
# Running fitdist() on normal dist and calculating p-value
normal_fit = fitdist(data_23$D_Score[data_23$Apparatus == "HB"], "norm")
summary(normal_fit)
calculate_p_value(normal_fit$estimate[[1]], normal_fit$estimate[[2]])

# Compare this to the sample mean and standard deviation
mean(data_23$D_Score[data_23$Apparatus == "HB"])
sd(data_23$D_Score[data_23$Apparatus == "HB"])
```

p-values seem really low, so we can plot this distribution over the data to see how it looks

```{r}
# Plot of the normal distribution over the plot of Apparatus == HB D_Score data
data_23 %>%
  filter(Apparatus == "HB") %>%
  ggplot(aes(x = D_Score)) +
  geom_density() +
  stat_function(fun = dnorm, args = list(normal_fit$estimate[[1]], normal_fit$estimate[[2]]), color = "red", size = 1) +
  labs(title = paste("HB Difficulty compared to normal distribution"),  
       x = "Difficulty Score",
       y = "Density")
```

Looks like a really good fit, but this has me wondering whether when calculating the posterior distribution, should we consider both the mean and the standard deviation to be known values?

```{r}
# Loop through all people who have done HB and calculate the standard deviations of their difficulty scores
HB_sd = data_23 %>%
  filter(Apparatus == "HB") %>%
  group_by(FirstName, LastName) %>%
  summarise(sd = var(D_Score)) %>%
  mutate(sd = ifelse(is.na(sd), 0, sd))

summary(HB_sd$sd)

# Plot histogram of SDs
HB_sd %>%
  filter(sd > 0) %>%
  ggplot(aes(x = sd)) +
  geom_histogram() +
  labs(title = "Distribution of SDs of Difficulty Scores",
       x = "SD",
       y = "Count") + 
  geom_vline(xintercept = 0.718**2, color = "red")
```

It seems like the standard deviations themselves follow an inv-gamma distribution, canonically the conjugate prior would be an inverse-gamma but I'm not sure how to approach that and it feels like it would be really complicated to have two prior distributions for difficulty score every apparatus for both genders. I also forgot if I should build the prior from the distribution of means/standard deviations or from the distribution of actual scores

```{r}
# Loop through all people who have done HB and calculate the mean of their difficulty scores
HB_mu = data_23 %>%
  filter(Apparatus == "HB") %>%
  group_by(FirstName, LastName, Gender, Country) %>%
  summarise(mu = mean(D_Score))

mean(HB_mu$mu)

# Plot histogram of means
HB_mu %>%
  ggplot(aes(x = mu)) +
  geom_histogram() +
  labs(title = "Distribution of Means of Difficulty Scores",
       x = "SD",
       y = "Count") + 
  geom_vline(xintercept = 4.87, color = "red")
```

I think the reason the mean looks so weird is because some athletes get sampled more than others, i don't care though. I'm going to fit a normal distribution to the means and an inv gamma distribution to the SDs and use those as priors

```{r}
normal_mu_fit = fitdist(HB_mu$mu, "norm")
invgamma_sd_fit = fitdist(HB_sd$sd[HB_sd$sd > 0], "invgamma")
```

```{r}
# Plot of the invgamma distribution over the plot HB sd scores
HB_sd %>%
  filter(sd > 0) %>%
  ggplot(aes(x = sd)) +
  geom_density() +
  stat_function(fun = dinvgamma, args = list(shape = invgamma_sd_fit$estimate[[1]],scale = invgamma_sd_fit$estimate[[2]]), color = "red", size = 1) +
  labs(title = paste("HB SD of difficulty scores compared to fitted invgamma distribution"),  
       x = "Difficulty Score",
       y = "Density")
```

Copying code from sta360 hw 4

```{r}
smc = 1000
m_0 =  normal_mu_fit$estimate[[1]]
sig_0 = normal_mu_fit$estimate[[2]]
k_0 = invgamma_sd_fit$estimate[[1]] 
v_0 = invgamma_sd_fit$estimate[[2]]

analyze_athlete = function(athlete_results, m_0, sig_0, k_0, v_0, smc) {
  n = length(athlete_results)
  ybar = mean(athlete_results)
  
  if (is.na(var(athlete_results))) {
    var = 0
  } else {
    var = var(athlete_results) 
  }
  
  k_n = k_0 + n
  v_n = v_0 + n
  m_n = (k_0*m_0 + n*ybar)/k_n
  sig_n = (v_0*sig_0 + (n-1)*var + k_0*n*(ybar - m_0)**2/k_n)/v_n
  sig_sample = mean(rinvgamma(smc, v_n/2, sig_n*v_n/2))
  theta_sample = mean(rnorm(smc, m_n, sqrt(sig_sample/k_n)))
  
  return(c(theta_sample, sig_sample))
}
```

```{r}
simul_data = data_23 %>%
  filter(Apparatus == "HB") %>%
  group_by(FirstName, LastName) %>%
  summarise(mean_estimate = analyze_athlete(D_Score, m_0 = m_0, sig_0 = sig_0, k_0 = k_0, v_0 = v_0, smc = 1000)[1],
            sd_estimate = analyze_athlete(D_Score, m_0 = m_0, sig_0 = sig_0, k_0 = k_0, v_0 = v_0, smc = 1000)[2]) %>%
  rowwise() %>%
  mutate(simulated_D_Score = rnorm(1, mean_estimate, sd_estimate))

simul_data %>%
  ggplot(aes(x = simulated_D_Score)) +
    geom_density(color = "red") +
    labs(title = "simulated d scores",  
         x = "Difficulty Score",
         y = "Density")
```

```{r}
data_23 %>%
  filter(Apparatus == "HB") %>% 
  group_by(FirstName, LastName) %>%
  summarise(mean_d_score = mean(D_Score),
            observations = n()) %>%
  filter(observations > 1) %>%
  inner_join(simul_data, by = c("FirstName", "LastName")) %>%
  ggplot() +
    geom_density(aes(x = mean_d_score), color = "blue") +
    geom_density(aes(x = simulated_D_Score), color = "red") +
    labs(title = "actual d scores vs simulated d scores",
         subtitle = "blue = actual, red = simulated",
         x = "Difficulty Score",
         y = "Density")
```



